{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"# step 1 >> Removing Noise \n#                --> number, punctuation, url\n# step 2 >> Transform any abnormalities\n#                --> spell correction\n#                --> demojization\n#                --> remove html tag\n#                --> process hashtags\n# step 3 >> Lowercase and split\n#                --> convert text to lowercase\n#                --> convert the text into a list of words\n# step 4 >> Remove stop words\n# step 5 >> stem the words\n# step 6 >> Join and returns\n#                --> join the list of words in text again\n#                --> return the joined text    ","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:10:22.672531Z","iopub.execute_input":"2021-10-17T23:10:22.67289Z","iopub.status.idle":"2021-10-17T23:10:22.683053Z","shell.execute_reply.started":"2021-10-17T23:10:22.672861Z","shell.execute_reply":"2021-10-17T23:10:22.682203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Domain specific pre-processing >> Libraries\n#                                   -->emoji() >> to convert emoji to text\n#                                   -->regex() >> to remove hashtags\n#                                   -->PyEnchant() >> for spell correction\n#                                   -->lxml() >> to remove html tags\n# --> removing the stop words i.e. the, me, and, my --> NLTK\n# --> stemming >> converts words to it's root form which is not in the vocabulari i.e. amazing becomes amaz after stemming --> NLTK\n# --> lemmatization >> does the same things as stemming; the only difference is that the lemmatized word is in the vocabulary --NLTK\n# --> removing the contraction >> I'm --> I am, He've --> He have","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:10:22.666825Z","iopub.execute_input":"2021-10-17T23:10:22.667248Z","iopub.status.idle":"2021-10-17T23:10:22.670759Z","shell.execute_reply.started":"2021-10-17T23:10:22.667206Z","shell.execute_reply":"2021-10-17T23:10:22.670119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Frequency Based Embedding >>\n#                          Type 1 >> Count Vectorizer \n#                                          --> convert a sentence into a bag of word representation\n#                          Type 2 >> TF-IDF Vectorizer\n#                                          --> convert a sentence into a vector","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:10:22.68399Z","iopub.execute_input":"2021-10-17T23:10:22.684627Z","iopub.status.idle":"2021-10-17T23:10:22.696749Z","shell.execute_reply.started":"2021-10-17T23:10:22.684597Z","shell.execute_reply":"2021-10-17T23:10:22.695875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis: Model","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\n\nfrom lxml import html      # --> to remove html tag\nfrom emoji import demojize  # --> to convert emoji into text\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:10:22.698328Z","iopub.execute_input":"2021-10-17T23:10:22.698748Z","iopub.status.idle":"2021-10-17T23:10:22.708142Z","shell.execute_reply.started":"2021-10-17T23:10:22.698703Z","shell.execute_reply":"2021-10-17T23:10:22.70708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sentimental-analysis-for-tweets/sentiment_tweets3.csv')\ndf.drop(['Index'], axis=1, inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:10:22.710674Z","iopub.execute_input":"2021-10-17T23:10:22.711004Z","iopub.status.idle":"2021-10-17T23:10:22.750015Z","shell.execute_reply.started":"2021-10-17T23:10:22.710962Z","shell.execute_reply":"2021-10-17T23:10:22.748977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns={'message to examine':'text', 'label (depression result)': 'sentiment'}, inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:10:22.751264Z","iopub.execute_input":"2021-10-17T23:10:22.751617Z","iopub.status.idle":"2021-10-17T23:10:22.762259Z","shell.execute_reply.started":"2021-10-17T23:10:22.75157Z","shell.execute_reply":"2021-10-17T23:10:22.761287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:10:22.763736Z","iopub.execute_input":"2021-10-17T23:10:22.76405Z","iopub.status.idle":"2021-10-17T23:10:22.774039Z","shell.execute_reply.started":"2021-10-17T23:10:22.764006Z","shell.execute_reply":"2021-10-17T23:10:22.773303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\nstop = stopwords.words('english')\n\ndef clean_text(text):\n    # Convert emoji to text\n    text = demojize(text)\n    \n    # Remove HTML Tags\n    try:\n        text = html.document_fromstring(text).text_content()\n    except:\n        pass\n    \n    # Remove hyperlinks\n    text = re.sub('http\\S+', ' ', text)\n    \n    # Remove non alphabets\n    text = re.sub('[^a-zA-Z ]+', ' ', text)\n    \n    # lowercase and stem\n    text = text.lower().split()\n    \n    # Remove stopwords and start words\n    text = [stemmer.stem(word) for word in text if word not in stop and len(word) > 2]\n    \n    # Join and Return\n    return ' '.join(text)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:18:53.811795Z","iopub.execute_input":"2021-10-17T23:18:53.812921Z","iopub.status.idle":"2021-10-17T23:18:53.820202Z","shell.execute_reply.started":"2021-10-17T23:18:53.812883Z","shell.execute_reply":"2021-10-17T23:18:53.819249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text = \"I'm learning Natural Language Processing\"\ncleaned_text = clean_text(sample_text)\ncleaned_text","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:18:55.070659Z","iopub.execute_input":"2021-10-17T23:18:55.070954Z","iopub.status.idle":"2021-10-17T23:18:55.079547Z","shell.execute_reply.started":"2021-10-17T23:18:55.07092Z","shell.execute_reply":"2021-10-17T23:18:55.078388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}